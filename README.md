# Vision Transformer (ViT) Model
This project focuses on developing a Vision Transformer (ViT) model to classify images from the Fashion MNIST dataset, which consists of 70,000 grayscale images of 28x28 pixels across 10 fashion categories, including clothing, footwear, and accessories. The Vision Transformer will leverage self-attention mechanisms to capture complex dependencies and patterns in the image data, splitting each image into a sequence of patches and processing them as tokens. The model will be trained using advanced optimization techniques to enhance its accuracy and generalization capabilities. The goal is to achieve state-of-the-art performance in classifying the Fashion MNIST images, showcasing the potential of transformer-based architectures in computer vision tasks.

## Code
The project code is present in [this notebook](https://github.com/rugvedmhatre/Vision-Transformers/blob/main/ViT-Fashion-MNIST-Classifier.ipynb)

## Results
Test Accuracy: **88.62%**
